{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 01 - Params\n",
    "\n",
    "url_base = \"https://www.twingalaxies.com/showthread.php/176004-Dispute-Jeremy-Young-Arcade-Donkey-Kong-Points-Hammer-Allowed-Player-Billy-L-Mitchell-Score-1-062-800/page\"\n",
    "file_name = \"./twingalaxies_dispute_billymitchell.xls\"\n",
    "file_name_linklist = \"./twingalaxies_dispute_billymitchell_linklist.csv\"\n",
    "\n",
    "# 02 - Definitions and imports\n",
    "\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import time\n",
    "import re\n",
    "import dateutil\n",
    "import collections\n",
    "import datetime\n",
    "\n",
    "def get_post_id(post_parent):\n",
    "        #print(post_parent)\n",
    "        try:\n",
    "            post_counter = post_parent.find(\"a\", {\"class\": \"postcounter\"})\n",
    "            return post_counter.contents[0]\n",
    "        except AttributeError:\n",
    "            return None\n",
    "\n",
    "        \n",
    "def get_thanks_or_likes(soup, str_thanks_or_likes):\n",
    "    out_dict = collections.defaultdict(list)\n",
    "    contents = soup.findAll('img', {\"alt\": str_thanks_or_likes})\n",
    "    for rec_content in contents:\n",
    "        if \"others</a>\" in str(rec_content.parent):\n",
    "            k = rec_content.parent.find(\"a\", {\"href\": re.compile(\"^thanks.php?\")})\n",
    "            url_thanks = \"https://www.twingalaxies.com/\" + k.attrs[\"href\"]\n",
    "\n",
    "            page_thanks = urllib.request.urlopen(url_thanks)\n",
    "            soup_thanks = bs4.BeautifulSoup(page_thanks, 'html.parser')\n",
    "\n",
    "            try:\n",
    "                hh = [x.contents[0] for x in soup_thanks.findAll(\"a\", {\"href\": re.compile(\"^member.php\")})]\n",
    "            except IndexError:\n",
    "                hh = [x.contents[0] for x in rec_content.parent.findAll(\"a\", {\"href\": re.compile(\"^member.php?\")})]\n",
    "        else:\n",
    "            hh = [x.contents[0] for x in rec_content.parent.findAll(\"a\", {\"href\": re.compile(\"^member.php?\")})]\n",
    "\n",
    "        out_list = [x if type(x) is bs4.element.NavigableString else x.contents[0] for x in hh]\n",
    "\n",
    "        parent_str = \"\"\n",
    "        post_id = None\n",
    "        while post_id is None:\n",
    "            parent_str += \".parent\"\n",
    "            exec_str = \"post_id = get_post_id(rec_content\" + parent_str + \")\"\n",
    "            ldict = locals()\n",
    "            exec(exec_str)\n",
    "            post_id = ldict[\"post_id\"]\n",
    "        \n",
    "        out_dict[post_id] = out_list\n",
    "        \n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def read_thread_df(file_name):\n",
    "    try:\n",
    "        all_loaded_df = pd.read_excel(file_name)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "        \n",
    "    for col_name in all_loaded_df.columns:\n",
    "        if \"_list\" in col_name:\n",
    "            all_loaded_df.loc[:, col_name] = all_loaded_df.loc[:, col_name].apply(lambda x: x.split(\", \") if not pd.isnull(x) else [])\n",
    "\n",
    "    return all_loaded_df\n",
    "\n",
    "\n",
    "def write_thread_df(all_df, file_name):\n",
    "\n",
    "    all_listasstrings_df = all_df.copy()\n",
    "\n",
    "    for col_name in all_listasstrings_df.columns:\n",
    "        if \"_list\" in col_name:\n",
    "            all_listasstrings_df.loc[:, col_name] = all_listasstrings_df.loc[:, col_name].apply(lambda x: re.sub(\"\\[|\\]|\\'\", \"\", str(x)))\n",
    "\n",
    "    all_listasstrings_df.to_excel(file_name, index=None)\n",
    "    all_listasstrings_df.to_csv(file_name.replace(\"xls\", \"csv\"), sep=\";\", quoting=2, encoding=\"utf-8\", index=None)\n",
    "\n",
    "\n",
    "def update_thread_df(all_df, url_base):\n",
    "    \n",
    "    def parse_postid_to_int(post_id):  \n",
    "        return int(post_id.replace(\"#\", \"\"))\n",
    "    \n",
    "    if all_df is None:\n",
    "        col_list = [\"post_id\", \"post_time\", \"url_post\", \"user_name\", \"post_score\", \"post_rank\", \"content\", \"links_list\", \"links_number\", \"cites_list\", \"cites_number\", \"thanks_list\", \"thanks_number\", \"likes_list\", \"likes_number\"]\n",
    "        all_df = pd.DataFrame(columns=col_list)\n",
    "        post_to_parse = 1\n",
    "        page_to_parse = 1\n",
    "    else:\n",
    "        post_last_read = all_df.tail(1)[\"post_id\"].iloc[0]\n",
    "        post_to_parse = parse_postid_to_int(post_last_read) + 1\n",
    "        page_to_parse = int(post_to_parse / 10) + 1\n",
    "\n",
    "    for page_number in range(page_to_parse, 1000, 1):\n",
    "\n",
    "        url_rec = url_base + str(page_number)\n",
    "        page = urllib.request.urlopen(url_rec)\n",
    "        soup = bs4.BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "        page_last_number = int(soup.findAll(\"a\", {\"class\": \"popupctrl jmppop\"})[0].contents[0].split(\" \")[-1])\n",
    "        if page_number > page_last_number:\n",
    "            break\n",
    "\n",
    "        print(\"Parsing page \" + str(page_number))\n",
    "        time.sleep(1)\n",
    "\n",
    "        contents = soup.findAll('div', id=re.compile('^post_message'))\n",
    "        users = soup.findAll('div', {\"class\": re.compile('msg-user comments')})\n",
    "        assert len(contents) == len(users)\n",
    "        rec_dict = {}\n",
    "\n",
    "        likes_list = get_thanks_or_likes(soup, \"Likes\")\n",
    "        thanks_list = get_thanks_or_likes(soup, \"Thanks\")\n",
    "\n",
    "        for post_id_in_page in range(len(contents)):\n",
    "\n",
    "            post_parent = contents[post_id_in_page].parent.parent.parent.parent.parent\n",
    "            post_counter = post_parent.find(\"a\", {\"class\": \"postcounter\"})\n",
    "            rec_dict[\"post_id\"] = get_post_id(post_parent)\n",
    "            if post_to_parse > parse_postid_to_int(rec_dict[\"post_id\"]):\n",
    "                continue\n",
    "\n",
    "            rec_dict[\"url_post\"] = \"https://www.twingalaxies.com/\" + post_counter.attrs[\"href\"]\n",
    "\n",
    "            d = [x for x in contents[post_id_in_page].contents if x != \"\\n\"][0]\n",
    "            e = [x for x in d.contents if x != \"\\n\"]\n",
    "            content_formatted = \"\\n\".join([str(x).replace(\"\\t\", \"\") for x in e])\n",
    "            content_formatted = re.sub(\"(^\\r\\n)|(\\r\\n$)|(\\r)\", \"\", content_formatted)\n",
    "            content_formatted = content_formatted.replace('<div class=\"bbcode_quote\">', \"\\n##### START OF QUOTE #####\\n\").replace(\"</div>\\n</div>\\n</div>\\n</div>\\n\", \"\\n##### END OF QUOTE #####\\n\")\n",
    "\n",
    "            content_soup = bs4.BeautifulSoup(content_formatted, \"lxml\")\n",
    "            for match in content_soup.findAll('span'):\n",
    "                match.unwrap()\n",
    "\n",
    "            content_formatted_nohtml = \" \".join(content_soup.strings)\n",
    "\n",
    "            rec_dict[\"content\"] = re.sub(\"\\n+\", \" \\n \", re.sub(\"^\\n|\\n$\", \"\", re.sub(\"[ ]*\\n+[ ]*\", \"\\n\", content_formatted_nohtml))) if isinstance(content_formatted_nohtml, str) else content_formatted_nohtml\n",
    "\n",
    "            content_no_double_spaces = rec_dict[\"content\"].replace(\"  \", \" \")\n",
    "            citations_in_str = [m.end() for m in re.finditer('Originally Posted by ', content_no_double_spaces)]\n",
    "            rec_dict[\"cites_list\"] = [content_no_double_spaces[idx:].split(\" \")[0] for idx in citations_in_str]\n",
    "            rec_dict[\"cites_number\"] = len(rec_dict[\"cites_list\"])\n",
    "\n",
    "            post_time_list = post_parent.find(\"span\", {\"class\": \"date\"}).contents\n",
    "\n",
    "            date_today = datetime.datetime.now()\n",
    "            date_yesterday = date_today - datetime.timedelta(days=1)\n",
    "            time_str = post_time_list[0].replace(\",\\xa0\", \"\") + \" \" +  post_time_list[1].contents[0]\n",
    "            time_str = time_str.replace(\"Yesterday\", date_yesterday.strftime(\"%Y-%m-%d\")).replace(\"Today\", date_today.strftime(\"%Y-%m-%d\"))\n",
    "            rec_dict[\"post_time\"] = dateutil.parser.parse(time_str)\n",
    "\n",
    "            user = [x for x in users[post_id_in_page].contents if x != \"\\n\"][0]\n",
    "            str_start = \".val('\"\n",
    "            idx_start = str(user).find(str_start) + len(str_start)\n",
    "            idx_end = idx_start + str(user)[idx_start:].find(\"'\")\n",
    "            rec_dict[\"user_name\"] = str(user)[idx_start:idx_end]\n",
    "\n",
    "            rec_dict[\"likes_list\"] = [x for x in likes_list[rec_dict[\"post_id\"]] if x != rec_dict[\"user_name\"]]\n",
    "            rec_dict[\"thanks_list\"] = [x for x in thanks_list[rec_dict[\"post_id\"]] if x != rec_dict[\"user_name\"]]\n",
    "\n",
    "            rec_dict[\"likes_number\"] = len(rec_dict[\"likes_list\"])\n",
    "            rec_dict[\"thanks_number\"] = len(rec_dict[\"thanks_list\"])\n",
    "\n",
    "            idx_start_list = [m.start() for m in re.finditer('href=\"htt', content_formatted)]\n",
    "            link_list = [content_formatted[idx:].split('\"')[1] for idx in idx_start_list]\n",
    "\n",
    "            idx_start_list = [m.start() for m in re.finditer('src=\"htt', content_formatted)]\n",
    "            link_list += [content_formatted[idx:].split('\"')[1] for idx in idx_start_list]\n",
    "\n",
    "            link_list_sel = [link for link in link_list if \"https://www.twingalaxies.com/showthread.php?p\" not in link]\n",
    "            rec_dict[\"links_list\"] = list(set(link_list_sel))\n",
    "\n",
    "            rec_dict[\"links_number\"] = len(rec_dict[\"links_list\"])\n",
    "\n",
    "            all_df = all_df.append(rec_dict, ignore_index=True)\n",
    "            \n",
    "        write_thread_df(all_df, file_name)\n",
    "            \n",
    "    return all_df\n",
    "\n",
    "# 03 - Programme\n",
    "\n",
    "all_df = read_thread_df(file_name)\n",
    "all_df = update_thread_df(all_df, url_base)\n",
    "write_thread_df(all_df, file_name)\n",
    "\n",
    "all_df.tail(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
